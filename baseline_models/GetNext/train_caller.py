import logging
import logging
import os
import pathlib
import pickle
import zipfile
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import yaml
from sklearn.preprocessing import OneHotEncoder
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from dataloader import load_graph_adj_mtx, load_graph_node_features
from model import GCN, NodeAttnMap, UserEmbeddings, Time2Vec, CategoryEmbeddings, FuseEmbeddings, TransformerModel
from param_parser import parameter_parser
from utils import increment_path, calculate_laplacian_matrix, zipdir, top_k_acc_last_timestep, \
    mAP_metric_last_timestep, MRR_metric_last_timestep, maksed_mse_loss, top_k_ndcg_last_timestep

global test_loader


def train(args):
    # args.save_dir = increment_path(Path(args.project) / args.name, exist_ok=args.exist_ok, sep='-')
    # if not os.path.exists(args.save_dir): os.makedirs(args.save_dir)

    # Setup logger
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    logging.basicConfig(level=logging.DEBUG,
                        format='%(asctime)s %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        filename=os.path.join("./exps/" + args.save_dir, f"log_training.txt"),
                        filemode='w')
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    console.setFormatter(formatter)
    logging.getLogger('').addHandler(console)
    logging.getLogger('matplotlib.font_manager').disabled = True

    # # Save run settings
    logging.info(args)
    with open(os.path.join("./exps/" + args.save_dir, 'args.yaml'), 'w') as f:
        yaml.dump(vars(args), f, sort_keys=False)

    # Save python code
    # zipf = zipfile.ZipFile(os.path.join(args.save_dir, 'code.zip'), 'w', zipfile.ZIP_DEFLATED)
    # zipdir(pathlib.Path().absolute(), zipf, include_format=['.py'])
    # zipf.close()

    # %% ====================== Load data ======================
    # Read check-in train data
    train_df = pd.read_csv(args.data_train)
    val_df = pd.read_csv(args.data_val)
    # test_df = pd.read_csv('/fast/xuxh/GETNext-master/dataset/NYC/NYC_test.csv')

    # Build POI graph (built from train_df)
    
    print('Loading POI graph...')
    raw_A = load_graph_adj_mtx(args.data_adj_mtx)
    raw_X = load_graph_node_features(args.data_node_feats,
                                     args.feature1,
                                     args.feature2,
                                     args.feature3,
                                     args.feature4)
    logging.info(
        f"raw_X.shape: {raw_X.shape}; "
        f"Four features: {args.feature1}, {args.feature2}, {args.feature3}, {args.feature4}.")
    logging.info(f"raw_A.shape: {raw_A.shape}; Edge from row_index to col_index with weight (frequency).")
    num_pois = raw_X.shape[0]

    # One-hot encoding poi categories
    logging.info('One-hot encoding poi categories id')
    one_hot_encoder = OneHotEncoder()
    cat_list = list(raw_X[:, 1])
    one_hot_encoder.fit(list(map(lambda x: [x], cat_list)))
    one_hot_rlt = one_hot_encoder.transform(list(map(lambda x: [x], cat_list))).toarray()
    num_cats = one_hot_rlt.shape[-1]
    X = np.zeros((num_pois, raw_X.shape[-1] - 1 + num_cats), dtype=np.float32)
    X[:, 0] = raw_X[:, 0]
    X[:, 1:num_cats + 1] = one_hot_rlt
    X[:, num_cats + 1:] = raw_X[:, 2:]
    logging.info(f"After one hot encoding poi cat, X.shape: {X.shape}")
    logging.info(f'POI categories: {list(one_hot_encoder.categories_[0])}')
    # Save ont-hot encoder
    with open(os.path.join("./exps/" + args.save_dir, 'one-hot-encoder.pkl'), "wb") as f:
        pickle.dump(one_hot_encoder, f)

    # Normalization
    print('Laplician matrix...')
    A = calculate_laplacian_matrix(raw_A, mat_type='hat_rw_normd_lap_mat')

    # POI id to index
    nodes_df = pd.read_csv(args.data_node_feats)
    poi_ids = list(set(nodes_df['node_name/poi_id'].tolist()))
    poi_id2idx_dict = dict(zip(poi_ids, range(len(poi_ids))))

    # Cat id to index
    cat_ids = list(set(nodes_df[args.feature2].tolist()))
    cat_id2idx_dict = dict(zip(cat_ids, range(len(cat_ids))))

    # Poi idx to cat idx
    poi_idx2cat_idx_dict = {}
    for i, row in nodes_df.iterrows():
        poi_idx2cat_idx_dict[poi_id2idx_dict[row['node_name/poi_id']]] = \
            cat_id2idx_dict[row[args.feature2]]

    # User id to index
    user_ids = [str(each) for each in list(set(train_df['user_id'].to_list()))]
    user_id2idx_dict = dict(zip(user_ids, range(len(user_ids))))

    # Print user-trajectories count
    traj_list = list(set(train_df['trajectory_id'].tolist()))

    # %% ====================== Define Dataset ======================
    class TrajectoryDatasetTrain(Dataset):
        def __init__(self, train_df):
            self.df = train_df
            self.traj_seqs = []  # traj id: user id + traj no.
            self.input_seqs = []
            self.label_seqs = []

            for traj_id in tqdm(set(train_df['trajectory_id'].tolist())):
                traj_df = train_df[train_df['trajectory_id'] == traj_id]
                poi_ids = traj_df['POI_id'].to_list()
                poi_idxs = [poi_id2idx_dict[each] for each in poi_ids]
                time_feature = traj_df[args.time_feature].to_list()

                input_seq = []
                label_seq = []
                for i in range(len(poi_idxs) - 1):
                    input_seq.append((poi_idxs[i], time_feature[i]))
                    label_seq.append((poi_idxs[i + 1], time_feature[i + 1]))

                if len(input_seq) < args.short_traj_thres:
                    continue

                self.traj_seqs.append(traj_id)
                self.input_seqs.append(input_seq)
                self.label_seqs.append(label_seq)

        def __len__(self):
            assert len(self.input_seqs) == len(self.label_seqs) == len(self.traj_seqs)
            return len(self.traj_seqs)

        def __getitem__(self, index):
            return (self.traj_seqs[index], self.input_seqs[index], self.label_seqs[index])

    class TrajectoryDatasetVal(Dataset):
        def __init__(self, df):
            self.df = df
            self.traj_seqs = []
            self.input_seqs = []
            self.label_seqs = []

            for traj_id in tqdm(set(df['trajectory_id'].tolist())):
                user_id = traj_id.split('_')[0]

                # Ignore user if not in training set
                if user_id not in user_id2idx_dict.keys():
                    continue

                # Ger POIs idx in this trajectory
                traj_df = df[df['trajectory_id'] == traj_id]
                poi_ids = traj_df['POI_id'].to_list()
                poi_idxs = []
                time_feature = traj_df[args.time_feature].to_list()

                for each in poi_ids:
                    if each in poi_id2idx_dict.keys():
                        poi_idxs.append(poi_id2idx_dict[each])
                    else:
                        # Ignore poi if not in training set
                        continue

                # Construct input seq and label seq
                input_seq = []
                label_seq = []
                for i in range(len(poi_idxs) - 1):
                    input_seq.append((poi_idxs[i], time_feature[i]))
                    label_seq.append((poi_idxs[i + 1], time_feature[i + 1]))


                # Ignore seq if too short
                if len(input_seq) < args.short_traj_thres:
                    continue

                self.input_seqs.append(input_seq)
                self.label_seqs.append(label_seq)
                self.traj_seqs.append(traj_id)


        def __len__(self):
            assert len(self.input_seqs) == len(self.label_seqs) == len(self.traj_seqs)
            return len(self.traj_seqs)

        def __getitem__(self, index):
            return (self.traj_seqs[index], self.input_seqs[index], self.label_seqs[index])

    # %% ====================== Define dataloader ======================
    print('Prepare dataloader...')
    train_dataset = TrajectoryDatasetTrain(train_df)
    val_dataset = TrajectoryDatasetVal(val_df)
    
                            

    # %% ====================== Build Models ======================
    # Model1: POI embedding model
    if isinstance(X, np.ndarray):
        X = torch.from_numpy(X)
        A = torch.from_numpy(A)
    X = X.to(device=args.device, dtype=torch.float)
    A = A.to(device=args.device, dtype=torch.float)

    args.gcn_nfeat = X.shape[1]
    poi_embed_model = GCN(ninput=args.gcn_nfeat,
                          nhid=args.gcn_nhid,
                          noutput=args.poi_embed_dim,
                          dropout=args.gcn_dropout)

    # Node Attn Model
    node_attn_model = NodeAttnMap(in_features=X.shape[1], nhid=args.node_attn_nhid, use_mask=False)

    # %% Model2: User embedding model, nn.embedding
    num_users = len(user_id2idx_dict)
    user_embed_model = UserEmbeddings(num_users, args.user_embed_dim)

    # %% Model3: Time Model
    time_embed_model = Time2Vec('sin', out_dim=args.time_embed_dim)

    # %% Model4: Category embedding model
    cat_embed_model = CategoryEmbeddings(num_cats, args.cat_embed_dim)

    # %% Model5: Embedding fusion models
    embed_fuse_model1 = FuseEmbeddings(args.user_embed_dim, args.poi_embed_dim)
    embed_fuse_model2 = FuseEmbeddings(args.time_embed_dim, args.cat_embed_dim)

    # %% Model6: Sequence model
    args.seq_input_embed = args.poi_embed_dim + args.user_embed_dim + args.time_embed_dim + args.cat_embed_dim
    seq_model = TransformerModel(num_pois,
                                 num_cats,
                                 args.seq_input_embed,
                                 args.transformer_nhead,
                                 args.transformer_nhid,
                                 args.transformer_nlayers,
                                 dropout=args.transformer_dropout)

    # Define overall loss and optimizer
    optimizer = optim.Adam(params=list(poi_embed_model.parameters()) +
                                  list(node_attn_model.parameters()) +
                                  list(user_embed_model.parameters()) +
                                  list(time_embed_model.parameters()) +
                                  list(cat_embed_model.parameters()) +
                                  list(embed_fuse_model1.parameters()) +
                                  list(embed_fuse_model2.parameters()) +
                                  list(seq_model.parameters()),
                           lr=args.lr,
                           weight_decay=args.weight_decay)

    criterion_poi = nn.CrossEntropyLoss(ignore_index=-1)  # -1 is padding
    criterion_cat = nn.CrossEntropyLoss(ignore_index=-1)  # -1 is padding
    criterion_time = maksed_mse_loss

    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 'min', verbose=True, factor=args.lr_scheduler_factor)

    # %% Tool functions for training
    def input_traj_to_embeddings(sample, poi_embeddings):
        # Parse sample
        traj_id = sample[0]
        input_seq = [each[0] for each in sample[1]]
        input_seq_time = [each[1] for each in sample[1]]
        input_seq_cat = [poi_idx2cat_idx_dict[each] for each in input_seq]

        # User to embedding
        user_id = traj_id.split('_')[0]
        user_idx = user_id2idx_dict[user_id]
        input = torch.LongTensor([user_idx]).to(device=args.device)
        user_embedding = user_embed_model(input)
        user_embedding = torch.squeeze(user_embedding)

        # POI to embedding and fuse embeddings
        input_seq_embed = []
        for idx in range(len(input_seq)):
            poi_embedding = poi_embeddings[input_seq[idx]]
            poi_embedding = torch.squeeze(poi_embedding).to(device=args.device)

            # Time to vector
            time_embedding = time_embed_model(
                torch.tensor([input_seq_time[idx]], dtype=torch.float).to(device=args.device))
            time_embedding = torch.squeeze(time_embedding).to(device=args.device)

            # Categroy to embedding
            cat_idx = torch.LongTensor([input_seq_cat[idx]]).to(device=args.device)
            cat_embedding = cat_embed_model(cat_idx)
            cat_embedding = torch.squeeze(cat_embedding)

            # Fuse user+poi embeds
            fused_embedding1 = embed_fuse_model1(user_embedding, poi_embedding)
            fused_embedding2 = embed_fuse_model2(time_embedding, cat_embedding)

            # Concat time, cat after user+poi
            concat_embedding = torch.cat((fused_embedding1, fused_embedding2), dim=-1)

            # Save final embed
            input_seq_embed.append(concat_embedding)

        return input_seq_embed

    def adjust_pred_prob_by_graph(y_pred_poi):
        y_pred_poi_adjusted = torch.zeros_like(y_pred_poi)
        attn_map = node_attn_model(X, A)

        for i in range(len(batch_seq_lens)):
            traj_i_input = batch_input_seqs[i]  # list of input check-in pois
            for j in range(len(traj_i_input)):
                y_pred_poi_adjusted[i, j, :] = attn_map[traj_i_input[j], :] + y_pred_poi[i, j, :]

        return y_pred_poi_adjusted
    # %% ====================== Test ======================

    if args.test==1:
        test_df = pd.read_csv(args.data_test)
        test_dataset = TrajectoryDatasetVal(val_df)
        logging.getLogger().setLevel(logging.INFO)
        val_loader = DataLoader(test_dataset,
                        batch_size=args.batch,
                        shuffle=False, drop_last=False,
                        pin_memory=True, num_workers=args.workers,
                        collate_fn=lambda x: x)
        for epoch in range(1):
            path="./exps/" + args.dataset_type + "/checkpoints/best_epoch.state.pt"
            model=torch.load(path,map_location=torch.device('cpu'))
            poi_embed_model.load_state_dict(model['poi_embed_state_dict'])
            node_attn_model.load_state_dict(model['node_attn_state_dict'])
            user_embed_model.load_state_dict(model['user_embed_state_dict'])
            time_embed_model.load_state_dict(model['time_embed_state_dict'])
            cat_embed_model.load_state_dict(model['cat_embed_state_dict'])
            embed_fuse_model1.load_state_dict(model['embed_fuse1_state_dict'])
            embed_fuse_model2.load_state_dict(model['embed_fuse2_state_dict'])
            seq_model.load_state_dict(model['seq_model_state_dict'])

            poi_embed_model = poi_embed_model.to(device=args.device)
            node_attn_model = node_attn_model.to(device=args.device)
            user_embed_model = user_embed_model.to(device=args.device)
            time_embed_model = time_embed_model.to(device=args.device)
            cat_embed_model = cat_embed_model.to(device=args.device)
            embed_fuse_model1 = embed_fuse_model1.to(device=args.device)
            embed_fuse_model2 = embed_fuse_model2.to(device=args.device)
            seq_model = seq_model.to(device=args.device)


            poi_embed_model.eval()
            node_attn_model.eval()
            user_embed_model.eval()
            time_embed_model.eval()
            cat_embed_model.eval()
            embed_fuse_model1.eval()
            embed_fuse_model2.eval()
            seq_model.eval()
            val_batches_top1_acc_list = []
            val_batches_top5_acc_list = []
            val_batches_top10_acc_list = []
            val_batches_top20_acc_list = []
            val_batches_mAP20_list = []
            val_batches_mrr_list = []
            val_batches_loss_list = []
            val_batches_poi_loss_list = []
            val_batches_time_loss_list = []
            val_batches_cat_loss_list = []

            val_batches_top1_ndcg_list = []
            val_batches_top5_ndcg_list = []
            val_batches_top10_ndcg_list = []
            val_batches_top20_ndcg_list = []
            num_sample=0

            src_mask = seq_model.generate_square_subsequent_mask(args.batch).to(args.device)
            for vb_idx, batch in enumerate(val_loader):
                if len(batch) != args.batch:
                    src_mask = seq_model.generate_square_subsequent_mask(len(batch)).to(args.device)

                # For padding
                batch_input_seqs = []
                batch_seq_lens = []
                batch_seq_embeds = []
                batch_seq_labels_poi = []
                batch_seq_labels_time = []
                batch_seq_labels_cat = []

                poi_embeddings = poi_embed_model(X, A)

                # Convert input seq to embeddings
                for sample in batch:
                    traj_id = sample[0]
                    input_seq = [each[0] for each in sample[1]]
                    label_seq = [each[0] for each in sample[2]]
                    input_seq_time = [each[1] for each in sample[1]]
                    label_seq_time = [each[1] for each in sample[2]]
                    label_seq_cats = [poi_idx2cat_idx_dict[each] for each in label_seq]
                    input_seq_embed = torch.stack(input_traj_to_embeddings(sample, poi_embeddings))
                    batch_seq_embeds.append(input_seq_embed)
                    batch_seq_lens.append(len(input_seq))
                    batch_input_seqs.append(input_seq)
                    batch_seq_labels_poi.append(torch.LongTensor(label_seq))
                    batch_seq_labels_time.append(torch.FloatTensor(label_seq_time))
                    batch_seq_labels_cat.append(torch.LongTensor(label_seq_cats))

                # Pad seqs for batch training
                batch_padded = pad_sequence(batch_seq_embeds, batch_first=True, padding_value=-1)
                label_padded_poi = pad_sequence(batch_seq_labels_poi, batch_first=True, padding_value=-1)
                label_padded_time = pad_sequence(batch_seq_labels_time, batch_first=True, padding_value=-1)
                label_padded_cat = pad_sequence(batch_seq_labels_cat, batch_first=True, padding_value=-1)

                # Feedforward
                x = batch_padded.to(device=args.device, dtype=torch.float)
                y_poi = label_padded_poi.to(device=args.device, dtype=torch.long)
                y_time = label_padded_time.to(device=args.device, dtype=torch.float)
                y_cat = label_padded_cat.to(device=args.device, dtype=torch.long)
                y_pred_poi, y_pred_time, y_pred_cat = seq_model(x, src_mask)

                # Graph Attention adjusted prob
                y_pred_poi_adjusted = adjust_pred_prob_by_graph(y_pred_poi)

                # Calculate loss
                loss_poi = criterion_poi(y_pred_poi_adjusted.transpose(1, 2), y_poi)
                loss_time = criterion_time(torch.squeeze(y_pred_time), y_time)
                loss_cat = criterion_cat(y_pred_cat.transpose(1, 2), y_cat)
                loss = loss_poi + loss_time * args.time_loss_weight + loss_cat

                # Performance measurement
                top1_acc = 0
                top5_acc = 0
                top10_acc = 0
                top20_acc = 0
                mAP20 = 0
                mrr = 0

                top1_ndcg = 0
                top5_ndcg = 0
                top10_ndcg = 0
                top20_ndcg = 0

                batch_label_pois = y_poi.detach().cpu().numpy()
                batch_pred_pois = y_pred_poi_adjusted.detach().cpu().numpy()
                batch_pred_times = y_pred_time.detach().cpu().numpy()
                batch_pred_cats = y_pred_cat.detach().cpu().numpy()
                for label_pois, pred_pois, seq_len in zip(batch_label_pois, batch_pred_pois, batch_seq_lens):
                    num_sample+=1
                    label_pois = label_pois[:seq_len]  # shape: (seq_len, )
                    pred_pois = pred_pois[:seq_len, :]  # shape: (seq_len, num_poi)
                    top1_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=1)
                    top5_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=5)
                    top10_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=10)
                    top20_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=20)
                    mAP20 += mAP_metric_last_timestep(label_pois, pred_pois, k=20)
                    mrr += MRR_metric_last_timestep(label_pois, pred_pois)

                    top1_ndcg += top_k_ndcg_last_timestep(label_pois, pred_pois, k=1)
                    top5_ndcg += top_k_ndcg_last_timestep(label_pois, pred_pois, k=5)
                    top10_ndcg += top_k_ndcg_last_timestep(label_pois, pred_pois, k=10)
                    top20_ndcg += top_k_ndcg_last_timestep(label_pois, pred_pois, k=20)

                val_batches_top1_acc_list.append(top1_acc / len(batch_label_pois))
                val_batches_top5_acc_list.append(top5_acc / len(batch_label_pois))
                val_batches_top10_acc_list.append(top10_acc / len(batch_label_pois))
                val_batches_top20_acc_list.append(top20_acc / len(batch_label_pois))
                val_batches_mAP20_list.append(mAP20 / len(batch_label_pois))
                val_batches_mrr_list.append(mrr / len(batch_label_pois))
                val_batches_top1_ndcg_list.append(top1_ndcg / len(batch_label_pois))
                val_batches_top5_ndcg_list.append(top5_ndcg / len(batch_label_pois))
                val_batches_top10_ndcg_list.append(top10_ndcg / len(batch_label_pois))
                val_batches_top20_ndcg_list.append(top20_ndcg / len(batch_label_pois))

                # val_batches_top1_acc_list.append(top1_acc)
                # val_batches_top5_acc_list.append(top5_acc)
                # val_batches_top10_acc_list.append(top10_acc)
                # val_batches_top20_acc_list.append(top20_acc)
                # val_batches_mAP20_list.append(mAP20)
                # val_batches_mrr_list.append(mrr)
                # val_batches_top1_ndcg_list.append(top1_ndcg)
                # val_batches_top5_ndcg_list.append(top5_ndcg)
                # val_batches_top10_ndcg_list.append(top10_ndcg)
                # val_batches_top20_ndcg_list.append(top20_ndcg)



                val_batches_loss_list.append(loss.detach().cpu().numpy())
                val_batches_poi_loss_list.append(loss_poi.detach().cpu().numpy())
                val_batches_time_loss_list.append(loss_time.detach().cpu().numpy())
                val_batches_cat_loss_list.append(loss_cat.detach().cpu().numpy())





                # Report validation progress
                if (vb_idx % (args.batch * 2)) == 0:
                    sample_idx = 0
                    batch_pred_pois_wo_attn = y_pred_poi.detach().cpu().numpy()
                    logging.info(f'batch:{vb_idx}, '
                                    f'val_batch_loss:{loss.item():.2f}, '
                                    f'val_batch_top1_acc:{top1_acc / len(batch_label_pois):.2f}, '
                                    f'val_move_loss:{np.mean(val_batches_loss_list):.2f} \n'
                                    f'val_move_poi_loss:{np.mean(val_batches_poi_loss_list):.2f} \n'
                                    f'val_move_time_loss:{np.mean(val_batches_time_loss_list):.2f} \n'
                                    f'val_move_top1_acc:{np.mean(val_batches_top1_acc_list):.4f} \n'
                                    f'val_move_top5_acc:{np.mean(val_batches_top5_acc_list):.4f} \n'
                                    f'val_move_top10_acc:{np.mean(val_batches_top10_acc_list):.4f} \n'
                                    f'val_move_top20_acc:{np.mean(val_batches_top20_acc_list):.4f} \n'
                                    f'val_move_mAP20:{np.mean(val_batches_mAP20_list):.4f} \n'
                                    f'val_move_MRR:{np.mean(val_batches_mrr_list):.4f} \n'

                                    f'val_move_top1_ndcg:{np.mean(val_batches_top1_ndcg_list):.4f} \n'
                                    f'val_move_top5_ndcg:{np.mean(val_batches_top5_ndcg_list):.4f} \n'
                                    f'val_move_top10_ndcg:{np.mean(val_batches_top10_ndcg_list):.4f} \n'
                                    f'val_move_top20_ndcg:{np.mean(val_batches_top20_ndcg_list):.4f} \n'

                                    f'traj_id:{batch[sample_idx][0]}\n'
                                    f'input_seq:{batch[sample_idx][1]}\n'
                                    f'label_seq:{batch[sample_idx][2]}\n'
                                    f'pred_seq_poi_wo_attn:{list(np.argmax(batch_pred_pois_wo_attn, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                                    f'pred_seq_poi:{list(np.argmax(batch_pred_pois, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                                    f'label_seq_cat:{[poi_idx2cat_idx_dict[each[0]] for each in batch[sample_idx][2]]}\n'
                                    f'pred_seq_cat:{list(np.argmax(batch_pred_cats, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                                    f'label_seq_time:{list(batch_seq_labels_time[sample_idx].numpy()[:batch_seq_lens[sample_idx]])}\n'
                                    f'pred_seq_time:{list(np.squeeze(batch_pred_times)[sample_idx][:batch_seq_lens[sample_idx]])} \n' +
                                    '=' * 100)
            val_epochs_top1_acc_list = []
            val_epochs_top5_acc_list = []
            val_epochs_top10_acc_list = []
            val_epochs_top20_acc_list = []
            val_epochs_mAP20_list = []
            val_epochs_mrr_list = []
            val_epochs_loss_list = []
            val_epochs_poi_loss_list = []
            val_epochs_time_loss_list = []
            val_epochs_cat_loss_list = []

            val_epochs_top1_ndcg_list = []
            val_epochs_top5_ndcg_list = []
            val_epochs_top10_ndcg_list = []
            val_epochs_top20_ndcg_list = []


            # For saving ckpt
            max_val_score = -np.inf


            epoch_val_top1_acc = np.mean(val_batches_top1_acc_list)
            epoch_val_top5_acc = np.mean(val_batches_top5_acc_list)
            epoch_val_top10_acc = np.mean(val_batches_top10_acc_list)
            epoch_val_top20_acc = np.mean(val_batches_top20_acc_list)
            epoch_val_mAP20 = np.mean(val_batches_mAP20_list)
            epoch_val_mrr = np.mean(val_batches_mrr_list)
            
            epoch_val_top1_ndcg = np.mean(val_batches_top1_ndcg_list)
            epoch_val_top5_ndcg = np.mean(val_batches_top5_ndcg_list)
            epoch_val_top10_ndcg = np.mean(val_batches_top10_ndcg_list)
            epoch_val_top20_ndcg = np.mean(val_batches_top20_ndcg_list)


            # epoch_val_top1_acc = np.sum(val_batches_top1_acc_list)/num_sample
            # epoch_val_top5_acc = np.sum(val_batches_top5_acc_list)/num_sample
            # epoch_val_top10_acc = np.sum(val_batches_top10_acc_list)/num_sample
            # epoch_val_top20_acc = np.sum(val_batches_top20_acc_list)/num_sample
            # epoch_val_mAP20 = np.sum(val_batches_mAP20_list)/num_sample
            # epoch_val_mrr = np.sum(val_batches_mrr_list)/num_sample
            
            # epoch_val_top1_ndcg = np.sum(val_batches_top1_ndcg_list)/num_sample
            # epoch_val_top5_ndcg = np.sum(val_batches_top5_ndcg_list)/num_sample
            # epoch_val_top10_ndcg = np.sum(val_batches_top10_ndcg_list)/num_sample
            # epoch_val_top20_ndcg = np.sum(val_batches_top20_ndcg_list)/num_sample


            epoch_val_loss = np.mean(val_batches_loss_list)
            epoch_val_poi_loss = np.mean(val_batches_poi_loss_list)
            epoch_val_time_loss = np.mean(val_batches_time_loss_list)
            epoch_val_cat_loss = np.mean(val_batches_cat_loss_list)



            # Save metrics to list
            val_epochs_poi_loss_list.append(epoch_val_poi_loss)
            val_epochs_time_loss_list.append(epoch_val_time_loss)
            val_epochs_cat_loss_list.append(epoch_val_cat_loss)
            val_epochs_top1_acc_list.append(epoch_val_top1_acc)
            val_epochs_top5_acc_list.append(epoch_val_top5_acc)
            val_epochs_top10_acc_list.append(epoch_val_top10_acc)
            val_epochs_top20_acc_list.append(epoch_val_top20_acc)
            val_epochs_mAP20_list.append(epoch_val_mAP20)
            val_epochs_mrr_list.append(epoch_val_mrr)


            val_epochs_top1_ndcg_list.append(epoch_val_top1_ndcg)
            val_epochs_top5_ndcg_list.append(epoch_val_top5_ndcg)
            val_epochs_top10_ndcg_list.append(epoch_val_top10_ndcg)
            val_epochs_top20_ndcg_list.append(epoch_val_top20_ndcg)

            # Monitor loss and score
            monitor_loss = epoch_val_loss
            # monitor_score = np.mean(epoch_val_top1_acc * 4 + epoch_val_top20_acc)
            monitor_score = np.mean(epoch_val_top1_acc)

            # Learning rate schuduler
            lr_scheduler.step(monitor_loss)

            # Print epoch results
            logging.info(f"Epoch test\n"
                        f"val_loss: {epoch_val_loss:.4f}, "
                        f"val_poi_loss: {epoch_val_poi_loss:.4f}, "
                        f"val_time_loss: {epoch_val_time_loss:.4f}, "
                        f"val_cat_loss: {epoch_val_cat_loss:.4f}, "
                        f"val_top1_acc:{epoch_val_top1_acc:.4f}, "
                        f"val_top5_acc:{epoch_val_top5_acc:.4f}, "
                        f"val_top10_acc:{epoch_val_top10_acc:.4f}, "
                        f"val_top20_acc:{epoch_val_top20_acc:.4f}, "
                        f"val_mAP20:{epoch_val_mAP20:.4f}, "
                        f"val_mrr:{epoch_val_mrr:.4f},"
                        
                        f"val_top1_ndcg:{epoch_val_top1_ndcg:.4f}, "
                        f"val_top5_ndcg:{epoch_val_top5_ndcg:.4f}, "
                        f"val_top10_ndcg:{epoch_val_top10_ndcg:.4f}, "
                        f"val_top20_ndcg:{epoch_val_top20_ndcg:.4f}"
                        
                        )

        print("---- Testing Finished ----")
        return
    

    print("---- creating training variables -----")
    train_loader = DataLoader(train_dataset,
                              batch_size=args.batch,
                              shuffle=True, drop_last=False,
                              pin_memory=True, num_workers=args.workers,
                              collate_fn=lambda x: x)
    val_loader = DataLoader(val_dataset,
                            batch_size=args.batch,
                            shuffle=False, drop_last=False,
                            pin_memory=True, num_workers=args.workers,
                            collate_fn=lambda x: x)
    test_df = pd.read_csv(args.data_test)
    test_dataset = TrajectoryDatasetVal(test_df)
    test_loader = DataLoader(test_dataset,
                        batch_size=args.batch,
                        shuffle=False, drop_last=False,
                        pin_memory=True, num_workers=args.workers,
                        collate_fn=lambda x: x)

    # %% ====================== Train ======================
    poi_embed_model = poi_embed_model.to(device=args.device)
    node_attn_model = node_attn_model.to(device=args.device)
    user_embed_model = user_embed_model.to(device=args.device)
    time_embed_model = time_embed_model.to(device=args.device)
    cat_embed_model = cat_embed_model.to(device=args.device)
    embed_fuse_model1 = embed_fuse_model1.to(device=args.device)
    embed_fuse_model2 = embed_fuse_model2.to(device=args.device)
    seq_model = seq_model.to(device=args.device)

    # %% Loop epoch
    # For plotting
    train_epochs_top1_acc_list = []
    train_epochs_top5_acc_list = []
    train_epochs_top10_acc_list = []
    train_epochs_top20_acc_list = []
    train_epochs_mAP20_list = []
    train_epochs_mrr_list = []
    train_epochs_loss_list = []
    train_epochs_poi_loss_list = []
    train_epochs_time_loss_list = []
    train_epochs_cat_loss_list = []
    val_epochs_top1_acc_list = []
    val_epochs_top5_acc_list = []
    val_epochs_top10_acc_list = []
    val_epochs_top20_acc_list = []
    val_epochs_mAP20_list = []
    val_epochs_mrr_list = []
    val_epochs_loss_list = []
    val_epochs_poi_loss_list = []
    val_epochs_time_loss_list = []
    val_epochs_cat_loss_list = []

    test_epochs_top1_acc_list = []
    test_epochs_top5_acc_list = []
    test_epochs_top10_acc_list = []
    test_epochs_top20_acc_list = []
    test_epochs_mAP20_list = []
    test_epochs_mrr_list = []
    test_epochs_loss_list = []
    test_epochs_poi_loss_list = []
    test_epochs_time_loss_list = []
    test_epochs_cat_loss_list = []

    # For saving ckpt
    max_val_score = -np.inf

    print("Epoch loop")
    for epoch in range(args.epochs):
        logging.info(f"{'*' * 50}Epoch:{epoch:03d}{'*' * 50}\n")
        poi_embed_model.train()
        node_attn_model.train()
        user_embed_model.train()
        time_embed_model.train()
        cat_embed_model.train()
        embed_fuse_model1.train()
        embed_fuse_model2.train()
        seq_model.train()

        train_batches_top1_acc_list = []
        train_batches_top5_acc_list = []
        train_batches_top10_acc_list = []
        train_batches_top20_acc_list = []
        train_batches_mAP20_list = []
        train_batches_mrr_list = []
        train_batches_loss_list = []
        train_batches_poi_loss_list = []
        train_batches_time_loss_list = []
        train_batches_cat_loss_list = []
        src_mask = seq_model.generate_square_subsequent_mask(args.batch).to(args.device)
        # Loop batch
        for b_idx, batch in enumerate(train_loader):
            if len(batch) != args.batch:
                src_mask = seq_model.generate_square_subsequent_mask(len(batch)).to(args.device)

            # For padding
            batch_input_seqs = []
            batch_seq_lens = []
            batch_seq_embeds = []
            batch_seq_labels_poi = []
            batch_seq_labels_time = []
            batch_seq_labels_cat = []

            poi_embeddings = poi_embed_model(X, A)

            # Convert input seq to embeddings
            for sample in batch:
                # sample[0]: traj_id, sample[1]: input_seq, sample[2]: label_seq
                traj_id = sample[0]
                input_seq = [each[0] for each in sample[1]]
                label_seq = [each[0] for each in sample[2]]
                input_seq_time = [each[1] for each in sample[1]]
                label_seq_time = [each[1] for each in sample[2]]
                label_seq_cats = [poi_idx2cat_idx_dict[each] for each in label_seq]
                input_seq_embed = torch.stack(input_traj_to_embeddings(sample, poi_embeddings))
                batch_seq_embeds.append(input_seq_embed)
                batch_seq_lens.append(len(input_seq))
                batch_input_seqs.append(input_seq)
                batch_seq_labels_poi.append(torch.LongTensor(label_seq))
                batch_seq_labels_time.append(torch.FloatTensor(label_seq_time))
                batch_seq_labels_cat.append(torch.LongTensor(label_seq_cats))

            # Pad seqs for batch training
            batch_padded = pad_sequence(batch_seq_embeds, batch_first=True, padding_value=-1)
            label_padded_poi = pad_sequence(batch_seq_labels_poi, batch_first=True, padding_value=-1)
            label_padded_time = pad_sequence(batch_seq_labels_time, batch_first=True, padding_value=-1)
            label_padded_cat = pad_sequence(batch_seq_labels_cat, batch_first=True, padding_value=-1)

            # Feedforward
            x = batch_padded.to(device=args.device, dtype=torch.float)
            y_poi = label_padded_poi.to(device=args.device, dtype=torch.long)
            y_time = label_padded_time.to(device=args.device, dtype=torch.float)
            y_cat = label_padded_cat.to(device=args.device, dtype=torch.long)
            y_pred_poi, y_pred_time, y_pred_cat = seq_model(x, src_mask)

            # Graph Attention adjusted prob
            y_pred_poi_adjusted = adjust_pred_prob_by_graph(y_pred_poi)

            loss_poi = criterion_poi(y_pred_poi_adjusted.transpose(1, 2), y_poi)
            loss_time = criterion_time(torch.squeeze(y_pred_time), y_time)
            loss_cat = criterion_cat(y_pred_cat.transpose(1, 2), y_cat)

            # Final loss
            loss = loss_poi + loss_time * args.time_loss_weight + loss_cat
            optimizer.zero_grad()
            loss.backward(retain_graph=True)
            optimizer.step()

            # Performance measurement
            top1_acc = 0
            top5_acc = 0
            top10_acc = 0
            top20_acc = 0
            mAP20 = 0
            mrr = 0
            batch_label_pois = y_poi.detach().cpu().numpy()
            batch_pred_pois = y_pred_poi_adjusted.detach().cpu().numpy()
            batch_pred_times = y_pred_time.detach().cpu().numpy()
            batch_pred_cats = y_pred_cat.detach().cpu().numpy()
            for label_pois, pred_pois, seq_len in zip(batch_label_pois, batch_pred_pois, batch_seq_lens):
                label_pois = label_pois[:seq_len]  # shape: (seq_len, )
                pred_pois = pred_pois[:seq_len, :]  # shape: (seq_len, num_poi)
                top1_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=1)
                top5_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=5)
                top10_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=10)
                top20_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=20)
                mAP20 += mAP_metric_last_timestep(label_pois, pred_pois, k=20)
                mrr += MRR_metric_last_timestep(label_pois, pred_pois)
            train_batches_top1_acc_list.append(top1_acc / len(batch_label_pois))
            train_batches_top5_acc_list.append(top5_acc / len(batch_label_pois))
            train_batches_top10_acc_list.append(top10_acc / len(batch_label_pois))
            train_batches_top20_acc_list.append(top20_acc / len(batch_label_pois))
            train_batches_mAP20_list.append(mAP20 / len(batch_label_pois))
            train_batches_mrr_list.append(mrr / len(batch_label_pois))
            train_batches_loss_list.append(loss.detach().cpu().numpy())
            train_batches_poi_loss_list.append(loss_poi.detach().cpu().numpy())
            train_batches_time_loss_list.append(loss_time.detach().cpu().numpy())
            train_batches_cat_loss_list.append(loss_cat.detach().cpu().numpy())

            # Report training progress
            if (b_idx % (args.batch * 5)) == 0:
                sample_idx = 0
                batch_pred_pois_wo_attn = y_pred_poi.detach().cpu().numpy()
                logging.info(f'Epoch:{epoch}, batch:{b_idx}, '
                            f'train_batch_loss:{loss.item():.2f}, '
                            f'train_batch_top1_acc:{top1_acc / len(batch_label_pois):.2f}, '
                            f'train_move_loss:{np.mean(train_batches_loss_list):.2f}\n'
                            f'train_move_poi_loss:{np.mean(train_batches_poi_loss_list):.2f}\n'
                            f'train_move_time_loss:{np.mean(train_batches_time_loss_list):.2f}\n'
                            f'train_move_top1_acc:{np.mean(train_batches_top1_acc_list):.4f}\n'
                            f'train_move_top5_acc:{np.mean(train_batches_top5_acc_list):.4f}\n'
                            f'train_move_top10_acc:{np.mean(train_batches_top10_acc_list):.4f}\n'
                            f'train_move_top20_acc:{np.mean(train_batches_top20_acc_list):.4f}\n'
                            f'train_move_mAP20:{np.mean(train_batches_mAP20_list):.4f}\n'
                            f'train_move_MRR:{np.mean(train_batches_mrr_list):.4f}\n'
                            f'traj_id:{batch[sample_idx][0]}\n'
                            f'input_seq: {batch[sample_idx][1]}\n'
                            f'label_seq:{batch[sample_idx][2]}\n'
                            f'pred_seq_poi_wo_attn:{list(np.argmax(batch_pred_pois_wo_attn, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'pred_seq_poi:{list(np.argmax(batch_pred_pois, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'label_seq_cat:{[poi_idx2cat_idx_dict[each[0]] for each in batch[sample_idx][2]]}\n'
                            f'pred_seq_cat:{list(np.argmax(batch_pred_cats, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'label_seq_time:{list(batch_seq_labels_time[sample_idx].numpy()[:batch_seq_lens[sample_idx]])}\n'
                            f'pred_seq_time:{list(np.squeeze(batch_pred_times)[sample_idx][:batch_seq_lens[sample_idx]])} \n' +
                            '=' * 100)

        # train end --------------------------------------------------------------------------------------------------------
        poi_embed_model.eval()
        node_attn_model.eval()
        user_embed_model.eval()
        time_embed_model.eval()
        cat_embed_model.eval()
        embed_fuse_model1.eval()
        embed_fuse_model2.eval()
        seq_model.eval()
        val_batches_top1_acc_list = []
        val_batches_top5_acc_list = []
        val_batches_top10_acc_list = []
        val_batches_top20_acc_list = []
        val_batches_mAP20_list = []
        val_batches_mrr_list = []
        val_batches_loss_list = []
        val_batches_poi_loss_list = []
        val_batches_time_loss_list = []
        val_batches_cat_loss_list = []
        src_mask = seq_model.generate_square_subsequent_mask(args.batch).to(args.device)
        for vb_idx, batch in enumerate(val_loader):
            if len(batch) != args.batch:
                src_mask = seq_model.generate_square_subsequent_mask(len(batch)).to(args.device)

            # For padding
            batch_input_seqs = []
            batch_seq_lens = []
            batch_seq_embeds = []
            batch_seq_labels_poi = []
            batch_seq_labels_time = []
            batch_seq_labels_cat = []

            poi_embeddings = poi_embed_model(X, A)

            # Convert input seq to embeddings
            for sample in batch:
                traj_id = sample[0]
                input_seq = [each[0] for each in sample[1]]
                label_seq = [each[0] for each in sample[2]]
                input_seq_time = [each[1] for each in sample[1]]
                label_seq_time = [each[1] for each in sample[2]]
                label_seq_cats = [poi_idx2cat_idx_dict[each] for each in label_seq]
                input_seq_embed = torch.stack(input_traj_to_embeddings(sample, poi_embeddings))
                batch_seq_embeds.append(input_seq_embed)
                batch_seq_lens.append(len(input_seq))
                batch_input_seqs.append(input_seq)
                batch_seq_labels_poi.append(torch.LongTensor(label_seq))
                batch_seq_labels_time.append(torch.FloatTensor(label_seq_time))
                batch_seq_labels_cat.append(torch.LongTensor(label_seq_cats))

            # Pad seqs for batch training
            batch_padded = pad_sequence(batch_seq_embeds, batch_first=True, padding_value=-1)
            label_padded_poi = pad_sequence(batch_seq_labels_poi, batch_first=True, padding_value=-1)
            label_padded_time = pad_sequence(batch_seq_labels_time, batch_first=True, padding_value=-1)
            label_padded_cat = pad_sequence(batch_seq_labels_cat, batch_first=True, padding_value=-1)

            # Feedforward
            x = batch_padded.to(device=args.device, dtype=torch.float)
            y_poi = label_padded_poi.to(device=args.device, dtype=torch.long)
            y_time = label_padded_time.to(device=args.device, dtype=torch.float)
            y_cat = label_padded_cat.to(device=args.device, dtype=torch.long)
            y_pred_poi, y_pred_time, y_pred_cat = seq_model(x, src_mask)

            # Graph Attention adjusted prob
            y_pred_poi_adjusted = adjust_pred_prob_by_graph(y_pred_poi)

            # Calculate loss
            loss_poi = criterion_poi(y_pred_poi_adjusted.transpose(1, 2), y_poi)
            loss_time = criterion_time(torch.squeeze(y_pred_time), y_time)
            loss_cat = criterion_cat(y_pred_cat.transpose(1, 2), y_cat)
            loss = loss_poi + loss_time * args.time_loss_weight + loss_cat

            # Performance measurement
            top1_acc = 0
            top5_acc = 0
            top10_acc = 0
            top20_acc = 0
            mAP20 = 0
            mrr = 0
            batch_label_pois = y_poi.detach().cpu().numpy()
            batch_pred_pois = y_pred_poi_adjusted.detach().cpu().numpy()
            batch_pred_times = y_pred_time.detach().cpu().numpy()
            batch_pred_cats = y_pred_cat.detach().cpu().numpy()
            for label_pois, pred_pois, seq_len in zip(batch_label_pois, batch_pred_pois, batch_seq_lens):
                label_pois = label_pois[:seq_len]  # shape: (seq_len, )
                pred_pois = pred_pois[:seq_len, :]  # shape: (seq_len, num_poi)
                top1_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=1)
                top5_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=5)
                top10_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=10)
                top20_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=20)
                mAP20 += mAP_metric_last_timestep(label_pois, pred_pois, k=20)
                mrr += MRR_metric_last_timestep(label_pois, pred_pois)
            val_batches_top1_acc_list.append(top1_acc / len(batch_label_pois))
            val_batches_top5_acc_list.append(top5_acc / len(batch_label_pois))
            val_batches_top10_acc_list.append(top10_acc / len(batch_label_pois))
            val_batches_top20_acc_list.append(top20_acc / len(batch_label_pois))
            val_batches_mAP20_list.append(mAP20 / len(batch_label_pois))
            val_batches_mrr_list.append(mrr / len(batch_label_pois))
            val_batches_loss_list.append(loss.detach().cpu().numpy())
            val_batches_poi_loss_list.append(loss_poi.detach().cpu().numpy())
            val_batches_time_loss_list.append(loss_time.detach().cpu().numpy())
            val_batches_cat_loss_list.append(loss_cat.detach().cpu().numpy())

            # Report validation progress
            if (vb_idx % (args.batch * 2)) == 0:
                sample_idx = 0
                batch_pred_pois_wo_attn = y_pred_poi.detach().cpu().numpy()
                logging.info(f'Epoch:{epoch}, batch:{vb_idx}, '
                            f'val_batch_loss:{loss.item():.2f}, '
                            f'val_batch_top1_acc:{top1_acc / len(batch_label_pois):.2f}, '
                            f'val_move_loss:{np.mean(val_batches_loss_list):.2f} \n'
                            f'val_move_poi_loss:{np.mean(val_batches_poi_loss_list):.2f} \n'
                            f'val_move_time_loss:{np.mean(val_batches_time_loss_list):.2f} \n'
                            f'val_move_top1_acc:{np.mean(val_batches_top1_acc_list):.4f} \n'
                            f'val_move_top5_acc:{np.mean(val_batches_top5_acc_list):.4f} \n'
                            f'val_move_top10_acc:{np.mean(val_batches_top10_acc_list):.4f} \n'
                            f'val_move_top20_acc:{np.mean(val_batches_top20_acc_list):.4f} \n'
                            f'val_move_mAP20:{np.mean(val_batches_mAP20_list):.4f} \n'
                            f'val_move_MRR:{np.mean(val_batches_mrr_list):.4f} \n'
                            f'traj_id:{batch[sample_idx][0]}\n'
                            f'input_seq:{batch[sample_idx][1]}\n'
                            f'label_seq:{batch[sample_idx][2]}\n'
                            f'pred_seq_poi_wo_attn:{list(np.argmax(batch_pred_pois_wo_attn, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'pred_seq_poi:{list(np.argmax(batch_pred_pois, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'label_seq_cat:{[poi_idx2cat_idx_dict[each[0]] for each in batch[sample_idx][2]]}\n'
                            f'pred_seq_cat:{list(np.argmax(batch_pred_cats, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'label_seq_time:{list(batch_seq_labels_time[sample_idx].numpy()[:batch_seq_lens[sample_idx]])}\n'
                            f'pred_seq_time:{list(np.squeeze(batch_pred_times)[sample_idx][:batch_seq_lens[sample_idx]])} \n' +
                            '=' * 100)
        # valid end --------------------------------------------------------------------------------------------------------

        #------------------------------------------------------------------------test--------------------------------------
        test_batches_top1_acc_list = []
        test_batches_top5_acc_list = []
        test_batches_top10_acc_list = []
        test_batches_top20_acc_list = []
        test_batches_mAP20_list = []
        test_batches_mrr_list = []
        test_batches_loss_list = []
        test_batches_poi_loss_list = []
        test_batches_time_loss_list = []
        test_batches_cat_loss_list = []
        src_mask = seq_model.generate_square_subsequent_mask(args.batch).to(args.device)
        for test_idx, batch in enumerate(test_loader):
            if len(batch) != args.batch:
                src_mask = seq_model.generate_square_subsequent_mask(len(batch)).to(args.device)

            # For padding
            batch_input_seqs = []
            batch_seq_lens = []
            batch_seq_embeds = []
            batch_seq_labels_poi = []
            batch_seq_labels_time = []
            batch_seq_labels_cat = []

            poi_embeddings = poi_embed_model(X, A)

            # Convert input seq to embeddings
            for sample in batch:
                traj_id = sample[0]
                input_seq = [each[0] for each in sample[1]]
                label_seq = [each[0] for each in sample[2]]
                input_seq_time = [each[1] for each in sample[1]]
                label_seq_time = [each[1] for each in sample[2]]
                label_seq_cats = [poi_idx2cat_idx_dict[each] for each in label_seq]
                input_seq_embed = torch.stack(input_traj_to_embeddings(sample, poi_embeddings))
                batch_seq_embeds.append(input_seq_embed)
                batch_seq_lens.append(len(input_seq))
                batch_input_seqs.append(input_seq)
                batch_seq_labels_poi.append(torch.LongTensor(label_seq))
                batch_seq_labels_time.append(torch.FloatTensor(label_seq_time))
                batch_seq_labels_cat.append(torch.LongTensor(label_seq_cats))

            # Pad seqs for batch training
            batch_padded = pad_sequence(batch_seq_embeds, batch_first=True, padding_value=-1)
            label_padded_poi = pad_sequence(batch_seq_labels_poi, batch_first=True, padding_value=-1)
            label_padded_time = pad_sequence(batch_seq_labels_time, batch_first=True, padding_value=-1)
            label_padded_cat = pad_sequence(batch_seq_labels_cat, batch_first=True, padding_value=-1)

            # Feedforward
            x = batch_padded.to(device=args.device, dtype=torch.float)
            y_poi = label_padded_poi.to(device=args.device, dtype=torch.long)
            y_time = label_padded_time.to(device=args.device, dtype=torch.float)
            y_cat = label_padded_cat.to(device=args.device, dtype=torch.long)
            y_pred_poi, y_pred_time, y_pred_cat = seq_model(x, src_mask)

            # Graph Attention adjusted prob
            y_pred_poi_adjusted = adjust_pred_prob_by_graph(y_pred_poi)

            # Calculate loss
            loss_poi = criterion_poi(y_pred_poi_adjusted.transpose(1, 2), y_poi)
            loss_time = criterion_time(torch.squeeze(y_pred_time), y_time)
            loss_cat = criterion_cat(y_pred_cat.transpose(1, 2), y_cat)
            loss = loss_poi + loss_time * args.time_loss_weight + loss_cat

            # Performance measurement
            top1_acc = 0
            top5_acc = 0
            top10_acc = 0
            top20_acc = 0
            mAP20 = 0
            mrr = 0
            batch_label_pois = y_poi.detach().cpu().numpy()
            batch_pred_pois = y_pred_poi_adjusted.detach().cpu().numpy()
            batch_pred_times = y_pred_time.detach().cpu().numpy()
            batch_pred_cats = y_pred_cat.detach().cpu().numpy()
            for label_pois, pred_pois, seq_len in zip(batch_label_pois, batch_pred_pois, batch_seq_lens):
                label_pois = label_pois[:seq_len]  # shape: (seq_len, )
                pred_pois = pred_pois[:seq_len, :]  # shape: (seq_len, num_poi)
                top1_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=1)
                top5_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=5)
                top10_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=10)
                top20_acc += top_k_acc_last_timestep(label_pois, pred_pois, k=20)
                mAP20 += mAP_metric_last_timestep(label_pois, pred_pois, k=20)
                mrr += MRR_metric_last_timestep(label_pois, pred_pois)
            test_batches_top1_acc_list.append(top1_acc / len(batch_label_pois))
            test_batches_top5_acc_list.append(top5_acc / len(batch_label_pois))
            test_batches_top10_acc_list.append(top10_acc / len(batch_label_pois))
            test_batches_top20_acc_list.append(top20_acc / len(batch_label_pois))
            test_batches_mAP20_list.append(mAP20 / len(batch_label_pois))
            test_batches_mrr_list.append(mrr / len(batch_label_pois))
            test_batches_loss_list.append(loss.detach().cpu().numpy())
            test_batches_poi_loss_list.append(loss_poi.detach().cpu().numpy())
            test_batches_time_loss_list.append(loss_time.detach().cpu().numpy())
            test_batches_cat_loss_list.append(loss_cat.detach().cpu().numpy())

            # Report validation progress
            if (test_idx % (args.batch * 2)) == 0:
                sample_idx = 0
                batch_pred_pois_wo_attn = y_pred_poi.detach().cpu().numpy()
                logging.info(f'Epoch:{epoch}, batch:{vb_idx}, '
                            f'test_batch_loss:{loss.item():.2f}, '
                            f'test_batch_top1_acc:{top1_acc / len(batch_label_pois):.2f}, '
                            f'test_move_loss:{np.mean(val_batches_loss_list):.2f} \n'
                            f'test_move_poi_loss:{np.mean(val_batches_poi_loss_list):.2f} \n'
                            f'test_move_time_loss:{np.mean(val_batches_time_loss_list):.2f} \n'
                            f'test_move_top1_acc:{np.mean(val_batches_top1_acc_list):.4f} \n'
                            f'test_move_top5_acc:{np.mean(val_batches_top5_acc_list):.4f} \n'
                            f'test_move_top10_acc:{np.mean(val_batches_top10_acc_list):.4f} \n'
                            f'test_move_top20_acc:{np.mean(val_batches_top20_acc_list):.4f} \n'
                            f'test_move_mAP20:{np.mean(val_batches_mAP20_list):.4f} \n'
                            f'test_move_MRR:{np.mean(val_batches_mrr_list):.4f} \n'
                            f'traj_id:{batch[sample_idx][0]}\n'
                            f'input_seq:{batch[sample_idx][1]}\n'
                            f'label_seq:{batch[sample_idx][2]}\n'
                            f'pred_seq_poi_wo_attn:{list(np.argmax(batch_pred_pois_wo_attn, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'pred_seq_poi:{list(np.argmax(batch_pred_pois, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'label_seq_cat:{[poi_idx2cat_idx_dict[each[0]] for each in batch[sample_idx][2]]}\n'
                            f'pred_seq_cat:{list(np.argmax(batch_pred_cats, axis=2)[sample_idx][:batch_seq_lens[sample_idx]])} \n'
                            f'label_seq_time:{list(batch_seq_labels_time[sample_idx].numpy()[:batch_seq_lens[sample_idx]])}\n'
                            f'pred_seq_time:{list(np.squeeze(batch_pred_times)[sample_idx][:batch_seq_lens[sample_idx]])} \n' +
                            '=' * 100)




        #-----------------------------------------------------------------------------------------------------------------

        # Calculate epoch metrics
        epoch_train_top1_acc = np.mean(train_batches_top1_acc_list)
        epoch_train_top5_acc = np.mean(train_batches_top5_acc_list)
        epoch_train_top10_acc = np.mean(train_batches_top10_acc_list)
        epoch_train_top20_acc = np.mean(train_batches_top20_acc_list)
        epoch_train_mAP20 = np.mean(train_batches_mAP20_list)
        epoch_train_mrr = np.mean(train_batches_mrr_list)
        epoch_train_loss = np.mean(train_batches_loss_list)
        epoch_train_poi_loss = np.mean(train_batches_poi_loss_list)
        epoch_train_time_loss = np.mean(train_batches_time_loss_list)
        epoch_train_cat_loss = np.mean(train_batches_cat_loss_list)
        epoch_val_top1_acc = np.mean(val_batches_top1_acc_list)
        epoch_val_top5_acc = np.mean(val_batches_top5_acc_list)
        epoch_val_top10_acc = np.mean(val_batches_top10_acc_list)
        epoch_val_top20_acc = np.mean(val_batches_top20_acc_list)
        epoch_val_mAP20 = np.mean(val_batches_mAP20_list)
        epoch_val_mrr = np.mean(val_batches_mrr_list)
        epoch_val_loss = np.mean(val_batches_loss_list)
        epoch_val_poi_loss = np.mean(val_batches_poi_loss_list)
        epoch_val_time_loss = np.mean(val_batches_time_loss_list)
        epoch_val_cat_loss = np.mean(val_batches_cat_loss_list)


        epoch_test_top1_acc = np.mean(test_batches_top1_acc_list)
        epoch_test_top5_acc = np.mean(test_batches_top5_acc_list)
        epoch_test_top10_acc = np.mean(test_batches_top10_acc_list)
        epoch_test_top20_acc = np.mean(test_batches_top20_acc_list)
        epoch_test_mAP20 = np.mean(test_batches_mAP20_list)
        epoch_test_mrr = np.mean(test_batches_mrr_list)
        epoch_test_loss = np.mean(test_batches_loss_list)
        epoch_test_poi_loss = np.mean(test_batches_poi_loss_list)
        epoch_test_time_loss = np.mean(test_batches_time_loss_list)
        epoch_test_cat_loss = np.mean(test_batches_cat_loss_list)

        # Save metrics to list
        train_epochs_loss_list.append(epoch_train_loss)
        train_epochs_poi_loss_list.append(epoch_train_poi_loss)
        train_epochs_time_loss_list.append(epoch_train_time_loss)
        train_epochs_cat_loss_list.append(epoch_train_cat_loss)
        train_epochs_top1_acc_list.append(epoch_train_top1_acc)
        train_epochs_top5_acc_list.append(epoch_train_top5_acc)
        train_epochs_top10_acc_list.append(epoch_train_top10_acc)
        train_epochs_top20_acc_list.append(epoch_train_top20_acc)
        train_epochs_mAP20_list.append(epoch_train_mAP20)
        train_epochs_mrr_list.append(epoch_train_mrr)
        val_epochs_loss_list.append(epoch_val_loss)
        val_epochs_poi_loss_list.append(epoch_val_poi_loss)
        val_epochs_time_loss_list.append(epoch_val_time_loss)
        val_epochs_cat_loss_list.append(epoch_val_cat_loss)
        val_epochs_top1_acc_list.append(epoch_val_top1_acc)
        val_epochs_top5_acc_list.append(epoch_val_top5_acc)
        val_epochs_top10_acc_list.append(epoch_val_top10_acc)
        val_epochs_top20_acc_list.append(epoch_val_top20_acc)
        val_epochs_mAP20_list.append(epoch_val_mAP20)
        val_epochs_mrr_list.append(epoch_val_mrr)


        test_epochs_loss_list.append(epoch_test_loss)
        test_epochs_poi_loss_list.append(epoch_test_poi_loss)
        test_epochs_time_loss_list.append(epoch_test_time_loss)
        test_epochs_cat_loss_list.append(epoch_test_cat_loss)
        test_epochs_top1_acc_list.append(epoch_test_top1_acc)
        test_epochs_top5_acc_list.append(epoch_test_top5_acc)
        test_epochs_top10_acc_list.append(epoch_test_top10_acc)
        test_epochs_top20_acc_list.append(epoch_test_top20_acc)
        test_epochs_mAP20_list.append(epoch_test_mAP20)
        test_epochs_mrr_list.append(epoch_test_mrr)

        # Monitor loss and score
        monitor_loss = epoch_val_loss
        # monitor_score = np.mean(epoch_val_top1_acc * 4 + epoch_val_top20_acc)
        monitor_score = np.mean(epoch_val_top1_acc)

        # Learning rate schuduler
        lr_scheduler.step(monitor_loss)

        # Print epoch results
        logging.info(f"Epoch {epoch}/{args.epochs}\n"
                    f"train_loss:{epoch_train_loss:.4f}, "
                    f"train_poi_loss:{epoch_train_poi_loss:.4f}, "
                    f"train_time_loss:{epoch_train_time_loss:.4f}, "
                    f"train_cat_loss:{epoch_train_cat_loss:.4f}, "
                    f"train_top1_acc:{epoch_train_top1_acc:.4f}, "
                    f"train_top5_acc:{epoch_train_top5_acc:.4f}, "
                    f"train_top10_acc:{epoch_train_top10_acc:.4f}, "
                    f"train_top20_acc:{epoch_train_top20_acc:.4f}, "
                    f"train_mAP20:{epoch_train_mAP20:.4f}, "
                    f"train_mrr:{epoch_train_mrr:.4f}\n"
                    f"val_loss: {epoch_val_loss:.4f}, "
                    f"val_poi_loss: {epoch_val_poi_loss:.4f}, "
                    f"val_time_loss: {epoch_val_time_loss:.4f}, "
                    f"val_cat_loss: {epoch_val_cat_loss:.4f}, "
                    f"val_top1_acc:{epoch_val_top1_acc:.4f}, "
                    f"val_top5_acc:{epoch_val_top5_acc:.4f}, "
                    f"val_top10_acc:{epoch_val_top10_acc:.4f}, "
                    f"val_top20_acc:{epoch_val_top20_acc:.4f}, "
                    f"val_mAP20:{epoch_val_mAP20:.4f}, "
                    f"val_mrr:{epoch_val_mrr:.4f},"

                    f"test_loss: {epoch_test_loss:.4f}, "
                    f"test_poi_loss: {epoch_test_poi_loss:.4f}, "
                    f"test_time_loss: {epoch_test_time_loss:.4f}, "
                    f"test_cat_loss: {epoch_test_cat_loss:.4f}, "
                    f"test_top1_acc:{epoch_test_top1_acc:.4f}, "
                    f"test_top5_acc:{epoch_test_top5_acc:.4f}, "
                    f"test_top10_acc:{epoch_test_top10_acc:.4f}, "
                    f"test_top20_acc:{epoch_test_top20_acc:.4f}, "
                    f"test_mAP20:{epoch_test_mAP20:.4f}, "
                    f"test_mrr:{epoch_test_mrr:.4f}")

        # Save poi and user embeddings
        if args.save_embeds:
            print("------- saving embedgins -------")
            embeddings_save_dir = os.path.join("./exps/" + args.save_dir, 'embeddings')
            if not os.path.exists(embeddings_save_dir): os.makedirs(embeddings_save_dir)
            # Save best epoch embeddings
            if monitor_score >= max_val_score:
                # Save poi embeddings
                poi_embeddings = poi_embed_model(X, A).detach().cpu().numpy()
                poi_embedding_list = []
                for poi_idx in range(len(poi_id2idx_dict)):
                    poi_embedding = poi_embeddings[poi_idx]
                    poi_embedding_list.append(poi_embedding)
                save_poi_embeddings = np.array(poi_embedding_list)
                np.save(os.path.join(embeddings_save_dir, 'saved_poi_embeddings'), save_poi_embeddings)
                # Save user embeddings
                user_embedding_list = []
                for user_idx in range(len(user_id2idx_dict)):
                    input = torch.LongTensor([user_idx]).to(device=args.device)
                    user_embedding = user_embed_model(input).detach().cpu().numpy().flatten()
                    user_embedding_list.append(user_embedding)
                user_embeddings = np.array(user_embedding_list)
                np.save(os.path.join(embeddings_save_dir, 'saved_user_embeddings'), user_embeddings)
                # Save cat embeddings
                cat_embedding_list = []
                for cat_idx in range(len(cat_id2idx_dict)):
                    input = torch.LongTensor([cat_idx]).to(device=args.device)
                    cat_embedding = cat_embed_model(input).detach().cpu().numpy().flatten()
                    cat_embedding_list.append(cat_embedding)
                cat_embeddings = np.array(cat_embedding_list)
                np.save(os.path.join(embeddings_save_dir, 'saved_cat_embeddings'), cat_embeddings)
                # Save time embeddings
                time_embedding_list = []
                for time_idx in range(args.time_units):
                    input = torch.FloatTensor([time_idx]).to(device=args.device)
                    time_embedding = time_embed_model(input).detach().cpu().numpy().flatten()
                    time_embedding_list.append(time_embedding)
                time_embeddings = np.array(time_embedding_list)
                np.save(os.path.join(embeddings_save_dir, 'saved_time_embeddings'), time_embeddings)

        # Save model state dict
        if args.save_weights:
            print("------- saving weights -------")
            state_dict = {
                'epoch': epoch,
                'poi_embed_state_dict': poi_embed_model.state_dict(),
                'node_attn_state_dict': node_attn_model.state_dict(),
                'user_embed_state_dict': user_embed_model.state_dict(),
                'time_embed_state_dict': time_embed_model.state_dict(),
                'cat_embed_state_dict': cat_embed_model.state_dict(),
                'embed_fuse1_state_dict': embed_fuse_model1.state_dict(),
                'embed_fuse2_state_dict': embed_fuse_model2.state_dict(),
                'seq_model_state_dict': seq_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'user_id2idx_dict': user_id2idx_dict,
                'poi_id2idx_dict': poi_id2idx_dict,
                'cat_id2idx_dict': cat_id2idx_dict,
                'poi_idx2cat_idx_dict': poi_idx2cat_idx_dict,
                'node_attn_map': node_attn_model(X, A),
                'args': args,
                'epoch_train_metrics': {
                    'epoch_train_loss': epoch_train_loss,
                    'epoch_train_poi_loss': epoch_train_poi_loss,
                    'epoch_train_time_loss': epoch_train_time_loss,
                    'epoch_train_cat_loss': epoch_train_cat_loss,
                    'epoch_train_top1_acc': epoch_train_top1_acc,
                    'epoch_train_top5_acc': epoch_train_top5_acc,
                    'epoch_train_top10_acc': epoch_train_top10_acc,
                    'epoch_train_top20_acc': epoch_train_top20_acc,
                    'epoch_train_mAP20': epoch_train_mAP20,
                    'epoch_train_mrr': epoch_train_mrr
                },
                'epoch_val_metrics': {
                    'epoch_val_loss': epoch_val_loss,
                    'epoch_val_poi_loss': epoch_val_poi_loss,
                    'epoch_val_time_loss': epoch_val_time_loss,
                    'epoch_val_cat_loss': epoch_val_cat_loss,
                    'epoch_val_top1_acc': epoch_val_top1_acc,
                    'epoch_val_top5_acc': epoch_val_top5_acc,
                    'epoch_val_top10_acc': epoch_val_top10_acc,
                    'epoch_val_top20_acc': epoch_val_top20_acc,
                    'epoch_val_mAP20': epoch_val_mAP20,
                    'epoch_val_mrr': epoch_val_mrr
                },
                'epoch_test_metrics': {
                    'epoch_test_loss': epoch_test_loss,
                    'epoch_test_poi_loss': epoch_test_poi_loss,
                    'epoch_test_time_loss': epoch_test_time_loss,
                    'epoch_test_cat_loss': epoch_test_cat_loss,
                    'epoch_test_top1_acc': epoch_test_top1_acc,
                    'epoch_test_top5_acc': epoch_test_top5_acc,
                    'epoch_test_top10_acc': epoch_test_top10_acc,
                    'epoch_test_top20_acc': epoch_test_top20_acc,
                    'epoch_test_mAP20': epoch_test_mAP20,
                    'epoch_test_mrr': epoch_test_mrr
                }
            }
            model_save_dir = os.path.join("./exps/" + args.save_dir, 'checkpoints')
            # Save best val score epoch
            if monitor_score >= max_val_score:
                if not os.path.exists(model_save_dir): os.makedirs(model_save_dir)
                torch.save(state_dict, rf"{model_save_dir}/best_epoch.state.pt")
                with open(rf"{model_save_dir}/best_epoch.txt", 'w') as f:
                    print(state_dict['epoch_val_metrics'], file=f)
                max_val_score = monitor_score

        # Save train/val metrics for plotting purpose
        with open(os.path.join("./exps/" + args.save_dir, 'metrics-train.txt'), "w") as f:
            print(f'train_epochs_loss_list={[float(f"{each:.4f}") for each in train_epochs_loss_list]}', file=f)
            print(f'train_epochs_poi_loss_list={[float(f"{each:.4f}") for each in train_epochs_poi_loss_list]}', file=f)
            print(f'train_epochs_time_loss_list={[float(f"{each:.4f}") for each in train_epochs_time_loss_list]}',
                file=f)
            print(f'train_epochs_cat_loss_list={[float(f"{each:.4f}") for each in train_epochs_cat_loss_list]}', file=f)
            print(f'train_epochs_top1_acc_list={[float(f"{each:.4f}") for each in train_epochs_top1_acc_list]}', file=f)
            print(f'train_epochs_top5_acc_list={[float(f"{each:.4f}") for each in train_epochs_top5_acc_list]}', file=f)
            print(f'train_epochs_top10_acc_list={[float(f"{each:.4f}") for each in train_epochs_top10_acc_list]}',
                file=f)
            print(f'train_epochs_top20_acc_list={[float(f"{each:.4f}") for each in train_epochs_top20_acc_list]}',
                file=f)
            print(f'train_epochs_mAP20_list={[float(f"{each:.4f}") for each in train_epochs_mAP20_list]}', file=f)
            print(f'train_epochs_mrr_list={[float(f"{each:.4f}") for each in train_epochs_mrr_list]}', file=f)
        with open(os.path.join("./exps/" + args.save_dir, 'metrics-val.txt'), "w") as f:
            print(f'val_epochs_loss_list={[float(f"{each:.4f}") for each in val_epochs_loss_list]}', file=f)
            print(f'val_epochs_poi_loss_list={[float(f"{each:.4f}") for each in val_epochs_poi_loss_list]}', file=f)
            print(f'val_epochs_time_loss_list={[float(f"{each:.4f}") for each in val_epochs_time_loss_list]}', file=f)
            print(f'val_epochs_cat_loss_list={[float(f"{each:.4f}") for each in val_epochs_cat_loss_list]}', file=f)
            print(f'val_epochs_top1_acc_list={[float(f"{each:.4f}") for each in val_epochs_top1_acc_list]}', file=f)
            print(f'val_epochs_top5_acc_list={[float(f"{each:.4f}") for each in val_epochs_top5_acc_list]}', file=f)
            print(f'val_epochs_top10_acc_list={[float(f"{each:.4f}") for each in val_epochs_top10_acc_list]}', file=f)
            print(f'val_epochs_top20_acc_list={[float(f"{each:.4f}") for each in val_epochs_top20_acc_list]}', file=f)
            print(f'val_epochs_mAP20_list={[float(f"{each:.4f}") for each in val_epochs_mAP20_list]}', file=f)
            print(f'val_epochs_mrr_list={[float(f"{each:.4f}") for each in val_epochs_mrr_list]}', file=f)

        with open(os.path.join("./exps/" + args.save_dir, 'metrics-test.txt'), "w") as f:
            print(f'test_epochs_loss_list={[float(f"{each:.4f}") for each in test_epochs_loss_list]}', file=f)
            print(f'test_epochs_poi_loss_list={[float(f"{each:.4f}") for each in test_epochs_poi_loss_list]}', file=f)
            print(f'test_epochs_time_loss_list={[float(f"{each:.4f}") for each in test_epochs_time_loss_list]}', file=f)
            print(f'test_epochs_cat_loss_list={[float(f"{each:.4f}") for each in test_epochs_cat_loss_list]}', file=f)
            print(f'test_epochs_top1_acc_list={[float(f"{each:.4f}") for each in test_epochs_top1_acc_list]}', file=f)
            print(f'test_epochs_top5_acc_list={[float(f"{each:.4f}") for each in test_epochs_top5_acc_list]}', file=f)
            print(f'test_epochs_top10_acc_list={[float(f"{each:.4f}") for each in test_epochs_top10_acc_list]}', file=f)
            print(f'test_epochs_top20_acc_list={[float(f"{each:.4f}") for each in test_epochs_top20_acc_list]}', file=f)
            print(f'test_epochs_mAP20_list={[float(f"{each:.4f}") for each in test_epochs_mAP20_list]}', file=f)
            print(f'test_epochs_mrr_list={[float(f"{each:.4f}") for each in test_epochs_mrr_list]}', file=f)
        
        print("---- training finished ------")

if __name__ == '__main__':
    args = parameter_parser()
    # The name of node features in NYC/graph_X.csv
    args.feature1 = 'checkin_cnt'
    args.feature2 = 'poi_catid_code'
    args.feature3 = 'latitude'
    args.feature4 = 'longitude'
    train(args)
